{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0658fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kshit\\anaconda3\\envs\\tf\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\kshit\\anaconda3\\envs\\tf\\lib\\site-packages\\numpy\\.libs\\libopenblas.EL2C6PLE4ZYW3ECEVIV3OXXGRN2NRFM2.gfortran-win_amd64.dll\n",
      "C:\\Users\\kshit\\anaconda3\\envs\\tf\\lib\\site-packages\\numpy\\.libs\\libopenblas.WCDJNK7YVMPZQ2ME2ZZHJJRJ3JIKNDB7.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import ast\n",
    "import json\n",
    "import nltk\n",
    "import time\n",
    "import spacy\n",
    "import openai\n",
    "import random\n",
    "import jellyfish\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from umap import UMAP\n",
    "import seaborn as sns\n",
    "from datetime import date,timedelta\n",
    "from bertopic import BERTopic\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.metrics import silhouette_score\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2216a904",
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_creation(sheet5,layer,multi_product,product=None):\n",
    "    for senti in ['Positive','Negative','Neutral']:\n",
    "        if multi_product:\n",
    "            a = sheet5[(sheet5.Sentiment==senti)&(sheet5.Product==product)]\n",
    "        else:\n",
    "            a = sheet5[sheet5.Sentiment==senti]\n",
    "        L2 = {}\n",
    "        if layer=='L2':\n",
    "            x = 'L3'\n",
    "        else:\n",
    "            x = 'L2'\n",
    "        #Calculating the layer values (count,avg) using its next layer vales    \n",
    "        for aspect in a[layer+' Cluster'].unique():\n",
    "            b = a[a[layer+' Cluster']==aspect]\n",
    "            d = {}\n",
    "            for cluster in b[x+' Cluster'].unique():\n",
    "                c = b[b[x+' Cluster']==cluster]\n",
    "                pcount = list(b[b[x+' Cluster']==cluster][x+' Phrase Count'])[0]\n",
    "                rcnt = list(b[b[x+' Cluster']==cluster][x+' Review Count'])[0]\n",
    "                prating = list(b[b[x+' Cluster']==cluster][x+' Rating (Phrase)'])[0]\n",
    "                rrat = list(b[b[x+' Cluster']==cluster][x+' Rating (Review)'])[0]\n",
    "                d[cluster] = [pcount,prating,rcnt,rrat]\n",
    "            p_total = 0\n",
    "            p_product = 0\n",
    "            r_total = 0\n",
    "            r_product = 0\n",
    "            for key,value in d.items():\n",
    "                try:\n",
    "                    p_total+=value[0]\n",
    "                    r_total+=value[2]\n",
    "                except:\n",
    "                    continue\n",
    "                p_product+=(value[0]*value[1])\n",
    "                r_product+=(value[2]*value[3])\n",
    "            try:\n",
    "                L2[aspect] = [p_total,p_product/p_total,r_total,r_product/r_total]\n",
    "            except:\n",
    "                continue\n",
    "        #Inserting them into sheet5\n",
    "        for aspect,value in L2.items():\n",
    "            if multi_product:\n",
    "                for i in sheet5[(sheet5.Product==product) & (sheet5.Sentiment==senti) & (sheet5[layer+' Cluster']==aspect)].index:\n",
    "                    sheet5[layer+' Phrase Count'][i] = value[0]\n",
    "                    sheet5[layer+' Rating (Phrase)'][i] = value[1]\n",
    "                    sheet5[layer+' Review Count'][i] = value[2]\n",
    "                    sheet5[layer+' Rating (Review)'][i] = value[3]\n",
    "            else:\n",
    "                for i in sheet5[(sheet5.Sentiment==senti) & (sheet5[layer+' Cluster']==aspect)].index:\n",
    "                    sheet5[layer+' Phrase Count'][i] = value[0]\n",
    "                    sheet5[layer+' Rating (Phrase)'][i] = value[1]\n",
    "                    sheet5[layer+' Review Count'][i] = value[2]\n",
    "                    sheet5[layer+' Rating (Review)'][i] = value[3]\n",
    "    print(layer+\" Completed\\n\")\n",
    "    return sheet5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ccb75feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary_l3(df1,multi_product=False):\n",
    "    #This code will generate the summary of L3 Clusters and store it in sheet5\n",
    "    if multi_product:\n",
    "        l = ['Product','Cluster','Sentiment']\n",
    "    else:\n",
    "        l = ['Cluster','Sentiment']\n",
    "    x = pd.DataFrame(df1.groupby(l).count()['Phrase'])\n",
    "    x.columns = ['L3 Phrase Count']\n",
    "    z = pd.DataFrame(df1.groupby(l).mean()['Rating'])\n",
    "    z.columns = ['L3 Rating (Phrase)']\n",
    "    a = pd.DataFrame(df1.groupby(l+['Review']).mean().groupby(l).mean()['Rating'])\n",
    "    a.columns = ['L3 Rating (Review)']\n",
    "    layer3 = x.join(z).join(a)\n",
    "    layer3 = layer3.reset_index()\n",
    "    layer3['L3 Review Count']=''\n",
    "    if multi_product:\n",
    "        for i in layer3.index:\n",
    "            layer3['L3 Review Count'][i] = df1[(df1.Product==layer3['Product'][i])&(df1.Cluster==layer3['Cluster'][i])&(df1.Sentiment==layer3['Sentiment'][i])]['Review ID'].nunique()\n",
    "    else:\n",
    "        for i in layer3.index:\n",
    "            layer3['L3 Review Count'][i] = df1[(df1.Cluster==layer3['Cluster'][i])&(df1.Sentiment==layer3['Sentiment'][i])]['Review ID'].nunique()\n",
    "    return layer3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87b6ec72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def id_creation(sheet5,layer,client):\n",
    "    if layer==1:\n",
    "        id = 1\n",
    "        for u_cluster in sheet5['L1 Cluster'].unique():\n",
    "            for i in sheet5[sheet5['L1 Cluster']==u_cluster].index:\n",
    "                sheet5['L1 Cluster ID'][i] = client+'L1I'+ str(id)\n",
    "            id+=1\n",
    "    elif layer==2 or layer==3:\n",
    "        a = 'L'+str(layer-1)+' Cluster ID'\n",
    "        b = 'L'+str(layer)+' Cluster'\n",
    "        id = 1\n",
    "        for ul1_id in sheet5[a].unique():\n",
    "            for ul2_cluster in sheet5[sheet5[a]==ul1_id][b].unique():\n",
    "                for i in sheet5[(sheet5[a]==ul1_id) & (sheet5[b]==ul2_cluster)].index:\n",
    "                    sheet5[b+' ID'][i] = client+'L'+str(layer)+'I'+ str(id)\n",
    "                id+=1\n",
    "    else:\n",
    "        id = 1\n",
    "        for i in sheet5.index:\n",
    "            sheet5['L4 ID'][i] = client+'L4I'+ str(id)\n",
    "            id+=1\n",
    "    return sheet5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7d2a466",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clustering(df1,n_neighbors,words_per_topic,ng_range,compress,nlp,tokenization,remove_stopwords,aspect):\n",
    "    docs = list(df1.loc[:, \"Phrase\"].values)\n",
    "    \n",
    "    # Lemmatization\n",
    "    for i in range(len(docs)):\n",
    "        doc = nlp(docs[i])\n",
    "        tokens = []\n",
    "        for token in doc:\n",
    "            tokens.append(token)\n",
    "        lemmatized_sentence = \" \".join([token.lemma_ for token in doc])\n",
    "        docs[i] = lemmatized_sentence\n",
    "\n",
    "    docs = pd.Series(docs).apply(lambda x: tokenization(x)) #Tokenization is done to remove stopwords\n",
    "    docs = docs.apply(lambda x:remove_stopwords(x))\n",
    "    for i in range(len(docs)):\n",
    "        docs[i] = \" \".join(docs[i])\n",
    "\n",
    "    #This code is used to only retain adverb, verb, adjective, noun, pronoun & 'X' part of speech words\n",
    "    #For more information about part of speech refer spacy website\n",
    "    lst = ['ADV','VERB','ADJ','NOUN','PROPN','X','NUM']\n",
    "\n",
    "    for i in range(len(docs)):\n",
    "        sent = nlp(docs[i])\n",
    "        text= []\n",
    "        for word in sent:\n",
    "            if not word.is_punct and word.pos_ in lst:#not word.is_stop and\n",
    "                text.append(word.lemma_) #ps.stem(word.lemma_)\n",
    "\n",
    "        #This code is used to convert some word into another. This is useful in Topic Modelling\n",
    "#         suffix = {'hydrating':'hydrate','hydrated':'hydrate','hydration':'hydrate'}\n",
    "#         for j in range(len(text)):\n",
    "#             for su,w in suffix.items():\n",
    "#                 if text[j].endswith(su):\n",
    "#                     text[j] = text[j][:-len(su)] + w\n",
    "\n",
    "        #This code is used to remove duplicate words from phrases\n",
    "        k = []\n",
    "        st = ' '.join(text)\n",
    "        for x in text:\n",
    "            if (st.count(x)>=1 and (x not in k)):\n",
    "                k.append(x)\n",
    "#                 k.sort()\n",
    "        docs[i] = ' '.join(k)\n",
    "\n",
    "    docs = pd.Series(docs)\n",
    "\n",
    "    #Removing empty phrases from docs [Those phrases which contains stopwords]\n",
    "    df1['cp'] = docs\n",
    "    df1 = df1[df1.cp.astype(bool)]\n",
    "    docs = docs[docs.astype(bool)]\n",
    "    df1.index=range(len(df1))\n",
    "    docs.index=range(len(docs))\n",
    "\n",
    "    #To enable compression while Topic Modelling\n",
    "    if compress:\n",
    "        compress=\"auto\"\n",
    "    else:\n",
    "        compress=None\n",
    "\n",
    "    try:\n",
    "        modal = BERTopic(\n",
    "#                      embedding_model = nlp,\n",
    "                     top_n_words = words_per_topic,\n",
    "                     calculate_probabilities = True,  #Used in Outlier Reduction\n",
    "                     vectorizer_model = TfidfVectorizer(  #This vectorizer takes the importance of a word or ngram into consideration using tfidf\n",
    "#                                              vocabulary = v.get_feature_names_out(),\n",
    "                                     ngram_range = ng_range,\n",
    "                                     stop_words=\"english\"\n",
    "                     ),\n",
    "                     nr_topics=compress,\n",
    "                     diversity=1, #To diversify the generated topics. Set to maximum\n",
    "                     umap_model=UMAP(n_neighbors=n_neighbors,random_state=2), #Dimensionality Reduction Model, random_state is defined to make BERTopic reproducible\n",
    "                     ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=True,bm25_weighting=True), #Used for topic compression\n",
    "                     language=\"english\",\n",
    "                     n_gram_range=ng_range) \n",
    "        topics, probs = modal.fit_transform(docs.values)\n",
    "\n",
    "        try:\n",
    "            #Reducing Outliers [Removing the -1 topic and Making Cleaner Topics as well]\n",
    "            topics = modal.reduce_outliers(docs.values, topics, probabilities=probs, strategy=\"probabilities\")\n",
    "            modal.update_topics(docs.values, topics=topics)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        #Storing clusters corresponding to each phrase in df1\n",
    "        df1['Cluster']=''\n",
    "#                     predicted_topics, predicted_probs = modal.transform(docs)\n",
    "        #This code is used to remove duplicate words inside a topic name\n",
    "        for i in range(len(df1)):\n",
    "            s = modal.get_topic(topics[i])[0][0]\n",
    "            l = s.split()\n",
    "            k = []\n",
    "            for x in l:\n",
    "                if (s.count(x)>=1 and (x not in k)):\n",
    "                    k.append(x)\n",
    "            df1['Cluster'][i] = ' '.join(k)\n",
    "            \n",
    "        df1['Cluster'] = df1['Cluster'].str.capitalize()\n",
    "        print(modal.get_topic_info())\n",
    "        print(pd.DataFrame(df1['Cluster'].value_counts()))\n",
    "    except Exception as e:\n",
    "        print('Aspect contains insufficient number of phrases for clustering. BERTopic Error.\\n')\n",
    "        print(e)\n",
    "        df1['Cluster']=aspect\n",
    "        \n",
    "    return df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e739f36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar_phrase(df1):\n",
    "    df1['L3 Cluster Phrase']=''\n",
    "    df1['sim'] = ''\n",
    "    df1.drop(['index'],1,inplace=True)\n",
    "#Calculating the mean similarity score corresponding to each phrase & then taking the phrase with max mean similarity score\n",
    "    for cluster in df1['Cluster'].unique():\n",
    "        for senti in df1[df1.Cluster==cluster]['Sentiment'].unique():\n",
    "            dfx = df1[(df1.Cluster==cluster)&(df1.Sentiment==senti)]\n",
    "            if len(dfx)==1:\n",
    "                for j in dfx.index:\n",
    "                    df1['L3 Cluster Phrase'][j] = dfx['Phrase'].iloc[0].strip().capitalize()                   \n",
    "                continue\n",
    "\n",
    "            for i in dfx.index:\n",
    "                sim=[]\n",
    "                for j in dfx.index:\n",
    "                    if i!=j:\n",
    "                        sim.append(jellyfish.levenshtein_distance(df1['Phrase'][i],df1['Phrase'][j]))\n",
    "                df1['sim'][i] = sum(sim)/len(sim)\n",
    "\n",
    "            m = df1[(df1.Cluster==cluster)&(df1.Sentiment==senti)]['sim'].min()\n",
    "            p = list(df1[(df1.Cluster==cluster)&(df1.Sentiment==senti)&(df1['sim']==m)]['Phrase'])[0]\n",
    "            for i in dfx.index:\n",
    "                df1['L3 Cluster Phrase'][i] = p.strip().capitalize()                   \n",
    "    print(df1.loc[:,['Cluster','Sentiment','L3 Cluster Phrase']].sort_values(['Cluster','Sentiment'],ascending=False).drop_duplicates(),'\\n')\n",
    "    return df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2cada4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_merge(l4,multi_product,min_l3_clusters=3,min_l3_phrases=10,percentage=10):\n",
    "    df = pd.DataFrame(columns = list(l4.columns)+['sim'])\n",
    "    if multi_product:\n",
    "        ps = l4['Product'].unique()\n",
    "    else:\n",
    "        ps = [1]\n",
    "    for p in ps:\n",
    "        if multi_product:\n",
    "            df1 = l4[l4['Product']==p]\n",
    "            l2s = df1['L2 Cluster ID'].unique()\n",
    "        else:\n",
    "            df1 = l4.copy()\n",
    "            l2s = df1['L2 Cluster ID'].unique()   \n",
    "        for l2 in l2s:\n",
    "            for senti in df1[df1['L2 Cluster ID']==l2]['Sentiment'].unique():\n",
    "                df2 = df1[(df1['L2 Cluster ID']==l2)&(df1['Sentiment']==senti)]\n",
    "                if df2['L3 Cluster ID'].nunique()<=min_l3_clusters:\n",
    "                    df = df.append(df2,ignore_index=True)\n",
    "                    continue\n",
    "\n",
    "                l3_phrases = df2.drop_duplicates(['L3 Cluster ID'])['L3 Phrase Count'].mean()\n",
    "                l3s = df2['L3 Cluster ID'].unique()\n",
    "                x = []\n",
    "                for l3 in l3s:\n",
    "                    cnt = df2[df2['L3 Cluster ID']==l3]['L3 Phrase Count'].iloc[0]\n",
    "                    if cnt<((percentage/100)*l3_phrases) or cnt<min_l3_phrases:\n",
    "                        x.append(l3)\n",
    "                if len(x)==0:\n",
    "                    df = df.append(df2,ignore_index=True)\n",
    "                    continue\n",
    "                df = df.append(df2[~df2['L3 Cluster ID'].isin(x)])\n",
    "                df2 = df2[df2['L3 Cluster ID'].isin(x)]\n",
    "                df2['L3 Phrase Count'] = len(df2)\n",
    "                df2['L3 Review Count'] = df2['Review ID'].nunique()\n",
    "                df2['L3 Rating (Phrase)'] = df2['Review Rating'].mean()\n",
    "                df2['L3 Rating (Review)'] = df2.drop_duplicates(['Review ID'])['Review Rating'].mean()    \n",
    "                df2['L3 Cluster'] = 'Miscellaneous'\n",
    "                df2['L3 Cluster ID'] = l2+'_misc_'+senti[0]\n",
    "                if len(df2)==1:\n",
    "                    for j in df2.index:\n",
    "                        df2['L3 Cluster Phrase'][j] = df2['L3 Cluster Phrase'].iloc[0].strip().capitalize()                   \n",
    "                    df = df.append(df2,ignore_index=True)\n",
    "                    continue\n",
    "                else:\n",
    "                    df2['sim']=''\n",
    "                    for i in df2.index:\n",
    "                        sim=[]\n",
    "                        for j in df2.index:\n",
    "                            if i!=j:\n",
    "                                sim.append(jellyfish.levenshtein_distance(df2['L4 Phrase'][i],df2['L4 Phrase'][j]))\n",
    "                        df2['sim'][i] = sum(sim)/len(sim)\n",
    "                    df2['L3 Cluster Phrase'] = df2[(df2['sim']==df2['sim'].min())]['L4 Phrase'].iloc[0].strip().capitalize()                   \n",
    "                df = df.append(df2,ignore_index=True)\n",
    "    return df.drop(['sim'],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7629ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detailed_output(df,map_dict,client,take_sentiment=True,flag_rating=4,multi_product=False,compress=True,merging=False,\n",
    "                    min_l3_clusters=3,min_l3_phrases=10,percentage=10,words_per_topic=5,n_neighbors=100,ng_range=(1,3),\n",
    "                    words_to_remove=[],date=date.today().strftime('%Y%m%d'),version='v1',create_exl=False):\n",
    "    ###################################Removing Outlier Clusters################################\n",
    "    df3 = df\n",
    "    #########################################sheet5##########################################\n",
    "    sheet5 = pd.DataFrame(columns=['Sentiment','L1 Cluster ID','L1 Cluster','L1 Phrase Count','L1 Review Count',\n",
    "                                   'L1 Rating (Phrase)','L1 Rating (Review)','L2 Cluster ID','L2 Cluster','L2 Phrase Count',\n",
    "                                   'L2 Review Count','L2 Rating (Phrase)','L2 Rating (Review)','L3 Cluster ID','L3 Cluster',\n",
    "                                   'L3 Cluster Phrase','L3 Phrase Count','L3 Review Count','L3 Rating (Review)',\n",
    "                                   'L3 Rating (Phrase)','L4 ID','L4 Phrase','Review ID','Review Rating'])\n",
    "    sheet5['Sentiment'] = df3.Sentiment\n",
    "    sheet5['L2 Cluster'] = df3.Aspect\n",
    "    sheet5['L4 Phrase'] = df3.Phrase\n",
    "    sheet5['Review'] = df3.Review\n",
    "    sheet5['Review ID'] = df3['Review ID']\n",
    "    sheet5['Review Rating'] = df3.Rating\n",
    "    sheet5['Date'] = pd.to_datetime(df3.Date)\n",
    "    if multi_product:\n",
    "        df3['Product'] = df['Review ID'].str.split('R',expand=True)[0] #.apply(lambda x:x[0]+x[2])#\n",
    "        sheet5['Product'] = df3['Product']\n",
    "#     sheet5 = sheet5.merge(daf.loc[:,['Review ID','Verified Purchase', 'Recommended',\n",
    "#            'Incentivized', 'Upvote', 'Downvote', 'AgeRange', 'SkinConcerns']],on='Review ID')\n",
    "    sheet5.fillna('',inplace=True)\n",
    "    print(\"Clustering Started\\n\")\n",
    "    \n",
    "    #######################################Clustering###########################################\n",
    "    print(\"BERTopic Clustering will take a while\")\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    stopwords = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your',\n",
    "     'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", \n",
    "    'its','itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", \n",
    "    'these','those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did',\n",
    "    'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', \n",
    "    'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down',\n",
    "    'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how',\n",
    "    'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'nor', 'only', 'own', 'same', 'so', 'than', 'too',\n",
    "    'very', 's', 't','can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', \n",
    "    'ain', 'aren', \"aren't\",'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven',\n",
    "     \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \n",
    "     \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", 'since','first', 'even',\n",
    "     'thisssss', 'could','really','always','bc','truly','literally','48','none','yet','actually','like','would','ever','issss',\n",
    "    'although','much','10','do','super','c','f','definitely','completely','totally','ski','ive']+words_to_remove # 'not','no'\n",
    "    def tokenization(text):\n",
    "        return word_tokenize(text)\n",
    "    def remove_stopwords(text):\n",
    "        output= [i for i in text if i.lower() not in stopwords]\n",
    "        return output\n",
    "\n",
    "    df3.dropna(inplace=True)\n",
    "#     v = CountVectorizer(ngram_range = ng_range,stop_words=\"english\")\n",
    "#     vd = v.fit_transform(list(df3.loc[:, \"Phrase\"].values))\n",
    "    \n",
    "    start_time = time.time()\n",
    "    #Doing clustering for every aspect in every sentiment\n",
    "    for senti in ['Positive','Negative']:\n",
    "        for aspect in df3.Aspect.unique():\n",
    "            if take_sentiment:\n",
    "                df1 = df3[(df3.Aspect==aspect)&(df3.Sentiment==senti)]\n",
    "                print('Sentiment:',senti,'Aspect:',aspect)\n",
    "            else:\n",
    "                df1 = df3[(df3.Aspect==aspect)]\n",
    "                print('Aspect:',aspect)\n",
    "            df1 = df1.reset_index()\n",
    "            if len(df1)==0:\n",
    "                print(\"There are no phrases for this aspect & sentiment.\")\n",
    "                continue\n",
    "\n",
    "            scores = {}\n",
    "            for x in words_per_topic:\n",
    "              for y in n_neighbors:\n",
    "                st = time.time()\n",
    "                df1 = clustering(df1,y,x,ng_range,compress,nlp,tokenization,remove_stopwords,aspect)\n",
    "                vectorizer = TfidfVectorizer()\n",
    "                dv = vectorizer.fit_transform(df1['Phrase'])\n",
    "                try:\n",
    "                    sc = silhouette_score(dv,df1['Cluster'])\n",
    "                    scores[sc] = [x,y]\n",
    "                    et = time.time()\n",
    "                    print(\"words_per_topic & n_neighbors:\",[x,y])\n",
    "                    print(\"silhouette_score:\",sc)\n",
    "                    print(\"Time Taken {:.2f} sec \\n\".format(et-st))\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "            max_score = max(scores.keys())\n",
    "            print(\"Max Score:\",max_score) \n",
    "            print(\"words_per_topic & n_neighbors:\",[scores[max_score][0],scores[max_score][1]])\n",
    "            df1 = clustering(df1,scores[max_score][1],scores[max_score][0],ng_range,compress,nlp,tokenization,remove_stopwords,aspect)                   \n",
    "            #This code will give us the most similar phrase inside each L3 Cluster\n",
    "            df1 = most_similar_phrase(df1)\n",
    "\n",
    "            #This code will generate the summary of L3 Clusters and store it in sheet5\n",
    "            layer3 = summary_l3(df1,multi_product)\n",
    "        \n",
    "            if take_sentiment:\n",
    "                s = sheet5[(sheet5['L2 Cluster']==aspect)&(sheet5.Sentiment==senti)]\n",
    "            else:\n",
    "                s = sheet5[(sheet5['L2 Cluster']==aspect)]\n",
    "                \n",
    "            for i in s.index:\n",
    "                try:\n",
    "                    if multi_product:\n",
    "                        x = df1[(df1.Product==sheet5['Product'][i])&(df1.Phrase==sheet5['L4 Phrase'][i])&(df1['Review ID']==sheet5['Review ID'][i])&(df1['Sentiment']==sheet5['Sentiment'][i])]\n",
    "                    else:\n",
    "                        x = df1[(df1.Phrase==sheet5['L4 Phrase'][i])&(df1['Review ID']==sheet5['Review ID'][i])&(df1['Sentiment']==sheet5['Sentiment'][i])]\n",
    "                    c = list(x['Cluster'])[0]\n",
    "                    st = list(x['Sentiment'])[0]\n",
    "                    sheet5['L3 Cluster'][i] = c\n",
    "                    sheet5['L3 Cluster Phrase'][i] = list(x['L3 Cluster Phrase'])[0]\n",
    "                    \n",
    "                    if multi_product:\n",
    "                        prod = list(x['Product'])[0]\n",
    "                        y = layer3[(layer3.Product==prod)&(layer3.Cluster==c)&(layer3.Sentiment==st)]\n",
    "                    else:\n",
    "                        y = layer3[(layer3.Cluster==c)&(layer3.Sentiment==st)]\n",
    "                    sheet5['L3 Phrase Count'][i] = list(y['L3 Phrase Count'])[0]\n",
    "                    sheet5['L3 Review Count'][i] = list(y['L3 Review Count'])[0]\n",
    "                    sheet5['L3 Rating (Phrase)'][i] = list(y['L3 Rating (Phrase)'])[0]\n",
    "                    sheet5['L3 Rating (Review)'][i] = list(y['L3 Rating (Review)'])[0]\n",
    "                except Exception as e:\n",
    "#                     print(c,e)\n",
    "                    pass\n",
    "            \n",
    "        if take_sentiment==False:\n",
    "            break\n",
    "    print(\"Clustering done after {:.2f} sec \\n\".format(time.time() - start_time))\n",
    "\n",
    "    sheet5 = sheet5[sheet5['L3 Cluster'].astype(bool)] #To remove empty L3 clusters from sheet5\n",
    "    \n",
    "    \n",
    "    ###############################Layer 2 & 1 Creation##############################################\n",
    "#     return sheet5\n",
    "    if multi_product:\n",
    "        ps = sheet5.Product.unique()\n",
    "        for product in ps:\n",
    "            sheet5 = layer_creation(sheet5,'L2',multi_product,product)\n",
    "    else:\n",
    "        sheet5 = layer_creation(sheet5,'L2',multi_product)\n",
    "    \n",
    "    #This code is used to map L2 Clusters to L1 Clusters\n",
    "    d = map_dict\n",
    "    for l2 in sheet5['L2 Cluster'].unique():\n",
    "        dfx = sheet5[sheet5['L2 Cluster']==l2]\n",
    "        l1 = ''\n",
    "        for key,values in dP1.items():\n",
    "            if l2 in values:\n",
    "                l1 = key\n",
    "                break\n",
    "        for i in dfx.index:\n",
    "            sheet5['L1 Cluster'][i] = l1\n",
    "        \n",
    "    if multi_product:\n",
    "        ps = sheet5.Product.unique()\n",
    "        for product in ps:\n",
    "            sheet5 = layer_creation(sheet5,'L1',multi_product,product)\n",
    "    else:\n",
    "        sheet5 = layer_creation(sheet5,'L1',multi_product)\n",
    "    \n",
    "    ###################################Id insertion & output excel formation################################\n",
    "    for la in range(1,5):\n",
    "        sheet5 = id_creation(sheet5,layer=la,client=client)\n",
    "    \n",
    "    if merging:\n",
    "        sheet5 = cluster_merge(sheet5,multi_product,min_l3_clusters,min_l3_phrases,percentage)\n",
    "    ###################################Output excel formation################################    \n",
    "    #Creating different tabs in detailed output from sheet5 [layer 4]\n",
    "    d1 = sheet5\n",
    "    d2 = sheet5.iloc[:,:20]\n",
    "    d3 = sheet5.iloc[:,:13]\n",
    "    d4 = sheet5.iloc[:,:7]\n",
    "    \n",
    "    #Sorting values\n",
    "    print(\"Sorting Started\\n\")\n",
    "    if multi_product:\n",
    "        d4['Product'] = d3['Product'] = d2['Product'] = d1['Product']\n",
    "#         d1 = d1.sort_values(['Product','L1 Cluster','L2 Cluster','Sentiment','L3 Phrase Count','L3 Cluster'],ascending=False)\n",
    "#         d2 = d2.sort_values(['Product','L1 Cluster','L2 Cluster','Sentiment','L3 Phrase Count','L3 Cluster'],ascending=False)\n",
    "#         d3 = d3.sort_values(['Product','L1 Cluster','Sentiment','L2 Phrase Count','L2 Cluster'],ascending=False)\n",
    "#         d4 = d4.sort_values(['Product','L1 Cluster','Sentiment'],ascending=False)\n",
    "        #Removing Duplicates\n",
    "        d2.drop_duplicates(['Product','Sentiment','L2 Cluster','L3 Cluster'],inplace=True)\n",
    "        d3.drop_duplicates(['Product','Sentiment','L2 Cluster'],inplace=True)\n",
    "        d4.drop_duplicates(['Product','Sentiment','L1 Cluster'],inplace=True)\n",
    "    else:\n",
    "#         d1 = d1.sort_values(['L1 Cluster','L2 Cluster','Sentiment','L3 Phrase Count','L3 Cluster'],ascending=False)\n",
    "#         d2 = d2.sort_values(['L1 Cluster','L2 Cluster','Sentiment','L3 Phrase Count','L3 Cluster'],ascending=False)\n",
    "#         d3 = d3.sort_values(['L1 Cluster','Sentiment','L2 Phrase Count','L2 Cluster'],ascending=False)\n",
    "#         d4 = d4.sort_values(['L1 Cluster','Sentiment'],ascending=False)\n",
    "        #Removing Duplicates\n",
    "        d2.drop_duplicates(['Sentiment','L2 Cluster','L3 Cluster'],inplace=True)\n",
    "        d3.drop_duplicates(['Sentiment','L2 Cluster'],inplace=True)\n",
    "        d4.drop_duplicates(['Sentiment','L1 Cluster'],inplace=True)\n",
    "    \n",
    "    #Flagging [used to determine wishlist etc.]\n",
    "    print(\"Flagging Started\\n\")\n",
    "    for x in [d1,d2]:\n",
    "        x['Flag']=''\n",
    "        for i in x.index:\n",
    "            if x['L3 Rating (Review)'][i]>=flag_rating:\n",
    "                x['Flag'][i] = x['Sentiment'][i][0]+'+'\n",
    "            else:\n",
    "                x['Flag'][i] = x['Sentiment'][i][0]+'-'\n",
    "        \n",
    "    #Creating output excel file\n",
    "    if create_exl:\n",
    "        if multi_product:\n",
    "            name = client+\"_product\"+\"_detailed_\"+date+version+\".xlsx\"\n",
    "        else:\n",
    "            name = client+\"_category\"+\"_detailed_\"+date+version+\".xlsx\"\n",
    "        path = r\"{fname}\".format(fname=name)\n",
    "        with pd.ExcelWriter(path) as engine:\n",
    "            d1.to_excel(excel_writer=engine,sheet_name='L4',index=False)\n",
    "            d2.to_excel(excel_writer=engine,sheet_name='L3',index=False)\n",
    "            d3.to_excel(excel_writer=engine,sheet_name='L2',index=False)\n",
    "            d4.to_excel(excel_writer=engine,sheet_name='L1',index=False)\n",
    "        print(\"Excel File with 4 sheets is created.\\n\")\n",
    "        print(\"File name:\",name)\n",
    "    dfs = [d1,d2,d3,d4]\n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd08faeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hard coded dictionaries are created to club L2 aspects together to form L1 Clusters\n",
    "dP1 = { 'Customer Demographics':['Value','Skin','Purchase','Motivation','Emotion','Demographic','Brand'],\n",
    "         'Attributes':['Time','Partner','Odor','Ingredients','Design','Color','Application'],\n",
    "        'Product Performance':['Finish','Coverage','Competitor']}\n",
    "\n",
    "dP4 = {'Performance':['Allergy','Performance','Skin'],\n",
    "         'Application':['Application','Texture'],\n",
    "        'Emotional':['Brand','Competitor','Purchase','Sentiment'],\n",
    "        'Product':['Demographic','Motivation','Odor','Packaging','Price']}\n",
    "\n",
    "dP5 = {'Product Performance':['Outcome','Usage','Activity','Trust','Customerservice','Quality'],\n",
    "      'Product Attributes':['Taste','Flavor','Value','Ingredient','Design','Odor','Smell'],\n",
    "      'Customer Details':['Demographic','Brand','Competitor','Motivation']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "778e22a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# l1 = detailed_output(df1,map_dict=dP1,client='foundation',take_sentiment=False,ng_range=(1,3),n_neighbors=15,\n",
    "#                      words_per_topic = 1,words_to_remove=[],multi_product=False,compress=True,create_exl=False,version='v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a78557",
   "metadata": {},
   "source": [
    "# End Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "475968e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_n_reviews_l3(df,n=5,year_duration=2,review_length=None,phrases_per_review=None):\n",
    "    #Filtering with year duration\n",
    "    df1= df[df['Date']>=pd.to_datetime(date.today()-relativedelta(years=year_duration))]\n",
    "    #Calculating the similarity of each phrase wrt the most common phrase\n",
    "    \n",
    "    if review_length!=None:\n",
    "        df1['Review Length'] = df1['Review'].apply(lambda x:len(x))\n",
    "        df1 = df1[df1['Review Length']>=review_length]\n",
    "    if phrases_per_review!=None:\n",
    "        df1['Phrases/Review'] = df1.groupby(['Review ID']).transform(\"count\")['L4 ID']\n",
    "        df1 = df1[df1['Phrases/Review']<=phrases_per_review]\n",
    "        \n",
    "    df1['sim']=''\n",
    "    for id in df1['L3 Cluster ID'].unique():\n",
    "        for senti in ['Positive','Negative']:\n",
    "            for idx in df1[(df1['L3 Cluster ID']==id)&(df1['Sentiment']==senti)].index:\n",
    "                df1['sim'][idx] = jellyfish.levenshtein_distance(df1['L3 Cluster Phrase'][idx],df1['L4 Phrase'][idx])\n",
    "            \n",
    "    #Creating the top_n_reviews dataframe\n",
    "    df2 = pd.DataFrame(columns=df1.columns)\n",
    "    for idx in df1['L3 Cluster ID'].unique():\n",
    "        for senti in ['Positive','Negative']:\n",
    "            s = len(df1[(df1['L3 Cluster ID']==idx)&(df1.Sentiment==senti)])\n",
    "            #Appending the most recent phrase\n",
    "#             try:\n",
    "#                 df2 = df2.append(df1[(df1['L3 Cluster ID']==idx)&(df1.Sentiment==senti)].sort_values(['Date'],ascending=False).iloc[0])\n",
    "#             except:\n",
    "#                 pass\n",
    "            #Appending rest n phrases in sorted order of similarity\n",
    "            df3 = df1[(df1['L3 Cluster ID']==idx)&(df1.Sentiment==senti)].sort_values(['sim'])\n",
    "            #&(df1['Review ID']!=df2.iloc[-1]['Review ID'])\n",
    "            df3.drop_duplicates(['L3 Cluster ID','Review ID'],keep='first',inplace=True)\n",
    "            df2 = df2.append(df3.iloc[:min(s,n)])\n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "178ae5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_n_reviews_l2(df,client=None,n=5,version='v1',create_exl=False):\n",
    "    df['sim']=''\n",
    "    for id in df['L3 Cluster ID'].unique():\n",
    "        for senti in ['Positive','Negative']:\n",
    "            for idx in df[(df['L3 Cluster ID']==id)&(df['Sentiment']==senti)].index:\n",
    "                df['sim'][idx] = -1*jellyfish.levenshtein_distance(df['L3 Cluster Phrase'][idx],df['L4 Phrase'][idx])\n",
    "                \n",
    "    df2 = pd.DataFrame(columns=df.columns)\n",
    "    for idx in df['L2 Cluster ID'].unique():\n",
    "        for senti in ['Positive','Negative']:\n",
    "            df1 = df[(df['L2 Cluster ID']==idx)&(df['Sentiment']==senti)].sort_values(['L3 Review Count','sim'],ascending=False)\n",
    "            if len(df1)==0:\n",
    "                continue\n",
    "            df2 = df2.append(df1.iloc[0])\n",
    "            rid,cid = [],[]\n",
    "            rid.append(df1.iloc[0]['Review ID'])\n",
    "            cid.append(df1.iloc[0]['L3 Cluster ID'])\n",
    "            cnt=n-1\n",
    "            while(cnt):\n",
    "                try:\n",
    "                    df2 = df2.append(df1[(~df1['Review ID'].isin(rid))&(~df1['L3 Cluster ID'].isin(cid))].iloc[0])\n",
    "                    rid.append(df2.iloc[-1]['Review ID'])\n",
    "                    cid.append(df2.iloc[-1]['L3 Cluster ID'])\n",
    "                    cnt-=1\n",
    "                except:\n",
    "                    break\n",
    "    if create_exl:\n",
    "        name = client+\"_top_\"+str(n)+\"_reviews_l2_\"+date.today().strftime('%Y%m%d')+version+\".xlsx\"\n",
    "        df2.to_excel(name,index=False)\n",
    "    return df2.drop(['sim'],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "51228241",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('protein_product_detailed_20230318v1.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0b6035eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n_reviews_l2(df).to_excel('protein_top_5_reviews_l2_category_20230318v1.xlsx',index=False)\n",
    "top_n_reviews_l3(df).to_excel('protein_top_5_reviews_l3_category_20230318v1.xlsx',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e75b7447",
   "metadata": {},
   "outputs": [],
   "source": [
    "daf = pd.DataFrame()\n",
    "for p in df['Product'].unique():\n",
    "    dfp = top_n_reviews_l2(df[df.Product==p])\n",
    "    daf = pd.concat([daf, dfp], ignore_index=True)\n",
    "# for i in daf[daf['L3 Cluster']!='Miscellaneous'].index:\n",
    "#     daf['Flag2'][i]=1\n",
    "daf.sort_values(['L2 Cluster ID','Product','Sentiment'],ascending=False).to_excel('protein_top_5_reviews_l2_product_20230318v1.xlsx',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
